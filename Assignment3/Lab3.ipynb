{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2.4422e+33  3.0803e-41 -1.6391e+31\n",
      " 4.5689e-41 -1.0024e+22  4.5689e-41\n",
      " 4.2558e+32  3.0803e-41  4.2558e+32\n",
      " 3.0803e-41  4.2558e+32  3.0803e-41\n",
      " 4.2566e+32  3.0803e-41 -1.6820e+31\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5 , 3)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x was initialized by Pytorch constructor. \n",
    "\n",
    "We just input the Tensor's size and it will create the Pytorch Tensor object with corresponded size.\n",
    "\n",
    "The type of x is torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7426  0.3589  0.0410\n",
      " 0.3826  0.8828  0.7766\n",
      " 0.0127  0.6751  0.9888\n",
      " 0.7520  0.3937  0.1745\n",
      " 0.3106  0.4066  0.9412\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "<class 'torch.FloatTensor'>\n",
      "\n",
      " 1.1073  0.0651  0.6458\n",
      "-0.6131  1.1254 -1.3151\n",
      " 1.5377  1.2427 -0.9218\n",
      "-0.6334  0.5704  1.7906\n",
      "-0.2935 -0.2466  0.0032\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5 , 3)\n",
    "print(y)\n",
    "print(type(y))\n",
    "y2 = torch.randn(5 , 3)\n",
    "print(y2)\n",
    "print(type(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random values in y are in a uniform distribution on the interval [0,1).\n",
    "\n",
    "The type of x is torch.Tensor.\n",
    "\n",
    "torch.randn(5,3) will create tensor with same size but the distribution will be standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2.4422e+33  3.0803e-41 -1.6391e+31\n",
      " 4.5689e-41 -1.0024e+22  4.5689e-41\n",
      " 4.2558e+32  3.0803e-41  4.2558e+32\n",
      " 3.0803e-41  4.2558e+32  3.0803e-41\n",
      " 4.2566e+32  3.0803e-41 -1.6820e+31\n",
      "[torch.DoubleTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.7426  0.3589  0.0410\n",
      " 0.3826  0.8828  0.7766\n",
      " 0.0127  0.6751  0.9888\n",
      " 0.7520  0.3937  0.1745\n",
      " 0.3106  0.4066  0.9412\n",
      "[torch.DoubleTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=x.double()\n",
    "y=y.double()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type displayed is torch.float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[ -0.1859 , 1.3970 , 0.5236] ,\n",
    "                [ 2.3854 , 0.0707 , 2.1970] ,\n",
    "                [ -0.3587 , 1.2359 , 1.8951] ,\n",
    "                [ -0.1189 , -0.1376 , 0.4647] ,\n",
    "                [ -1.8968 , 2.0164 , 0.1092]])\n",
    "y = torch.Tensor([[ 0.4838 , 0.5822 , 0.2755] ,\n",
    "                [ 1.0982 , 0.4932 , -0.6680] ,\n",
    "                [ 0.7915 , 0.6580 , -0.5819] ,\n",
    "                [ 0.3825 , -1.1822 , 1.5217] ,\n",
    "                [ 0.6042 , -0.2280 , 1.3210]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack((x,y))\n",
    "z2 = torch.cat((x,y),0)\n",
    "z3 = torch.cat((x,y),1)\n",
    "print(z.shape)\n",
    "print(z2.shape)\n",
    "print(z3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32099997997\n",
      "1.32099997997\n"
     ]
    }
   ],
   "source": [
    "print(y[4,2])\n",
    "print(z[1,4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1092\n",
      " 1.3210\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(z[:,4,2])\n",
    "print(len(z[:,4,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x+y)\n",
    "print(torch.add(x,y))\n",
    "print(x.add(y))\n",
    "torch.add(x,y,out=x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are printing the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "print(x.size(),y.size(),z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view(16) squeezes x into one dimension.\n",
    "\n",
    "view(-1,8) reshape x to (xxx, 8) size where xxx is determined by its original size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-19.8243 -12.3032\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(10,10)\n",
    "y=torch.randn(2,100)\n",
    "x2=x.view(1,-1)\n",
    "y2=y.view(-1,2)\n",
    "res=torch.mm(x2,y2)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'torch.FloatTensor'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(5)\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)\n",
    "\n",
    "print(type(a))\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[2. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a[0]+=1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match. They share their underlying  memory locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      " [3. 2. 2. 2. 2.]\n",
      "\n",
      " 4\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 5]\n",
      " [4. 3. 3. 3. 3.]\n",
      "\n",
      " 5\n",
      " 4\n",
      " 4\n",
      " 4\n",
      " 4\n",
      "[torch.FloatTensor of size 5]\n",
      " [4. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a,b)\n",
    "a[:]+=1\n",
    "print(a,b)\n",
    "a=a.add(1)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them will modify the value of a.\n",
    "\n",
    "But a.add_(1) and a[:]+=1 will also modify the value of b. a.add(1) will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=np.ones(5)\n",
    "b=torch.from_numpy(a)\n",
    "np.add(a,1,out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6352  0.2053  2.4652\n",
      " 3.0973 -1.5817 -0.6140\n",
      " 0.0272 -1.4052  1.1294\n",
      "-0.3573  1.6090  2.2152\n",
      " 0.2849  1.0794 -3.4635\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n",
      "\n",
      " 0.6352  0.2053  2.4652\n",
      " 3.0973 -1.5817 -0.6140\n",
      " 0.0272 -1.4052  1.1294\n",
      "-0.3573  1.6090  2.2152\n",
      " 0.2849  1.0794 -3.4635\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(5,3)\n",
    "y=torch.randn(5,3)\n",
    "if torch.cuda.is_available():\n",
    "    x=x.cuda()\n",
    "    y=y.cuda()\n",
    "    z=x+y\n",
    "print(z)\n",
    "if torch.cuda.is_available():\n",
    "    print(z.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y and calculate z=x+y.\n",
    "\n",
    "If gpu is available, then transfer tensor to cuda tensor firstly and then calculate it by gpu.\n",
    "\n",
    "z.cpu() could also transfer z from cuda tensor to cpu tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6352043   0.20527676  2.4652302 ]\n",
      " [ 3.0973043  -1.5816753  -0.6140161 ]\n",
      " [ 0.02719772 -1.4051727   1.1294484 ]\n",
      " [-0.35725167  1.6089787   2.2151823 ]\n",
      " [ 0.28489646  1.0793769  -3.463544  ]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-11d5c486e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "print(z.cpu().numpy())\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the second statement is invalid for Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as ag\n",
    "x=ag.Variable(torch.ones(2,2),requires_grad=True)\n",
    "print(x)\n",
    "y=x+2\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z=y*y*3\n",
    "f=z.mean()\n",
    "print(z,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f = \\frac{1}{n} \\sum_{i=1}^{n}3(x_{i}+2)^{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\triangledown_{x}f(x_{i})=\\frac{6(x_{i}+2)}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "Based on the equation, the gradient should be 18/4 = 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.5000  4.5000\n",
       " 4.5000  4.5000\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results indicates that the gradient obtained by Pytorch is the same as our mathematical derivation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n",
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.transpose(xtrain.reshape(28,28,1,60000),[3,2,0,1])\n",
    "xtest = np.transpose(xtest.reshape(28,28,1,10000),[3,2,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as pyplot\n",
    "MNISTtools.show(xtrain[42,0,:,:])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that our preprocess is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as ag\n",
    "xtrain=ag.Variable(torch.from_numpy(xtrain),requires_grad=True).double()\n",
    "ltrain=ag.Variable(torch.from_numpy(ltrain),requires_grad=False).double()\n",
    "xtest=ag.Variable(torch.from_numpy(xtest),requires_grad=False).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input  =  1 x 28 x 28\n",
    "\n",
    "### conv1  =  6 x 24 x 24\n",
    " \n",
    "### pool1  =  6 x 12 x 12\n",
    "   \n",
    "### conv2   = 16 x 8 x 8\n",
    "\n",
    "### pool2  =  16 x 4 x 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# This is our neural networks class that inherits from nn. Module\n",
    "class LeNet(nn.Module):\n",
    "# Here we define our network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5).double()\n",
    "        self.conv2=nn.Conv2d(6,16,5).double()\n",
    "        self.fc1=nn.Linear(16*4*4,120).double()\n",
    "        self.fc2=nn.Linear(120,84).double()\n",
    "        self.fc3=nn.Linear(84,10).double()\n",
    "    # Here we define one forward pass through the network\n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    # Determine the number of features in a batch of tensors\n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:]\n",
    "        return np.prod(size)\n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 5, 5])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 5, 5])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 256])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params=list(net.parameters())\n",
    "for i in range(len(params)):\n",
    "    print(i,params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The params for indicies 0 and 2 are [num of output channels, num of input channels, kernel width, kernel height], which corresponds to the kernels for conv layers.\n",
    "\n",
    "The params for indicies 4,6,8 are the num of input features and the num of output features for fully connected layers.\n",
    "\n",
    "The params for the odd indices are bias for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.32\n"
     ]
    }
   ],
   "source": [
    "yinit = net(xtest)\n",
    "print(100*np.mean(ltest == yinit.data.numpy().T.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = xtrain.size()[0] # Training set size\n",
    "B = 100 # Minibacth size\n",
    "NB = 600 # Number of minibatches\n",
    "T = 10 # Number of epochs\n",
    "gamma = .001 # learning rate\n",
    "rho = .9 # momentum\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr = gamma,\n",
    "                            momentum = rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.219\n",
      "[1,   200] loss: 0.252\n",
      "[1,   300] loss: 0.167\n",
      "[1,   400] loss: 0.157\n",
      "[1,   500] loss: 0.125\n",
      "[1,   600] loss: 0.125\n",
      "[2,   100] loss: 0.100\n",
      "[2,   200] loss: 0.101\n",
      "[2,   300] loss: 0.087\n",
      "[2,   400] loss: 0.094\n",
      "[2,   500] loss: 0.080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-76ecf1db2eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Parameter update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB) # shuffling\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k] # index of minibatch\n",
    "        # Extract i-th minibatch from xtrain and ltrain\n",
    "        idxsmp = range(i*B,i*B+B) # indices of samples for i-th minibatch\n",
    "        inputs = xtrain[idxsmp]\n",
    "        labels = ltrain[idxsmp]\n",
    "        # Initialize the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = net(inputs)\n",
    "        # Error evaluation\n",
    "        loss = criterion(outputs,labels.long())\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "        # Print averaged loss per minibatch every 100 mini - batches\n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f'%\n",
    "                    (epoch + 1,k + 1,running_loss*1.0/100))\n",
    "            running_loss = 0.0\n",
    "print ('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "\n",
    "xtrain = np.transpose(xtrain.reshape(28,28,1,60000),[3,2,0,1])\n",
    "xtest = np.transpose(xtest.reshape(28,28,1,10000),[3,2,0,1])\n",
    "\n",
    "import torch.autograd as ag\n",
    "if torch.cuda.is_available():\n",
    "    xtrain=ag.Variable(torch.from_numpy(xtrain).cuda(),requires_grad=True).double()\n",
    "    ltrain=ag.Variable(torch.from_numpy(ltrain).cuda(),requires_grad=False).double()\n",
    "    xtest=ag.Variable(torch.from_numpy(xtest).cuda(),requires_grad=False).double()\n",
    "else:\n",
    "    xtrain=ag.Variable(torch.from_numpy(xtrain),requires_grad=True).double()\n",
    "    ltrain=ag.Variable(torch.from_numpy(ltrain),requires_grad=False).double()\n",
    "    xtest=ag.Variable(torch.from_numpy(xtest),requires_grad=False).double()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# This is our neural networks class that inherits from nn. Module\n",
    "class LeNet(nn.Module):\n",
    "# Here we define our network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5).double()\n",
    "        self.conv2=nn.Conv2d(6,16,5).double()\n",
    "        self.fc1=nn.Linear(16*4*4,120).double()\n",
    "        self.fc2=nn.Linear(120,84).double()\n",
    "        self.fc3=nn.Linear(84,10).double()\n",
    "    \n",
    "    # Here we define one forward pass through the network\n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # Determine the number of features in a batch of tensors\n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:]\n",
    "        return np.prod(size)\n",
    "    \n",
    "net = LeNet()\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.840\n",
      "[1,   200] loss: 0.241\n",
      "[1,   300] loss: 0.159\n",
      "[1,   400] loss: 0.149\n",
      "[1,   500] loss: 0.148\n",
      "[1,   600] loss: 0.121\n",
      "[2,   100] loss: 0.096\n",
      "[2,   200] loss: 0.090\n",
      "[2,   300] loss: 0.089\n",
      "[2,   400] loss: 0.082\n",
      "[2,   500] loss: 0.082\n",
      "[2,   600] loss: 0.082\n",
      "[3,   100] loss: 0.066\n",
      "[3,   200] loss: 0.067\n",
      "[3,   300] loss: 0.069\n",
      "[3,   400] loss: 0.054\n",
      "[3,   500] loss: 0.060\n",
      "[3,   600] loss: 0.053\n",
      "[4,   100] loss: 0.045\n",
      "[4,   200] loss: 0.051\n",
      "[4,   300] loss: 0.053\n",
      "[4,   400] loss: 0.040\n",
      "[4,   500] loss: 0.049\n",
      "[4,   600] loss: 0.045\n",
      "[5,   100] loss: 0.049\n",
      "[5,   200] loss: 0.034\n",
      "[5,   300] loss: 0.039\n",
      "[5,   400] loss: 0.040\n",
      "[5,   500] loss: 0.043\n",
      "[5,   600] loss: 0.037\n",
      "[6,   100] loss: 0.030\n",
      "[6,   200] loss: 0.032\n",
      "[6,   300] loss: 0.036\n",
      "[6,   400] loss: 0.033\n",
      "[6,   500] loss: 0.037\n",
      "[6,   600] loss: 0.033\n",
      "[7,   100] loss: 0.028\n",
      "[7,   200] loss: 0.028\n",
      "[7,   300] loss: 0.027\n",
      "[7,   400] loss: 0.029\n",
      "[7,   500] loss: 0.030\n",
      "[7,   600] loss: 0.032\n",
      "[8,   100] loss: 0.021\n",
      "[8,   200] loss: 0.034\n",
      "[8,   300] loss: 0.026\n",
      "[8,   400] loss: 0.026\n",
      "[8,   500] loss: 0.024\n",
      "[8,   600] loss: 0.027\n",
      "[9,   100] loss: 0.016\n",
      "[9,   200] loss: 0.022\n",
      "[9,   300] loss: 0.025\n",
      "[9,   400] loss: 0.026\n",
      "[9,   500] loss: 0.024\n",
      "[9,   600] loss: 0.021\n",
      "[10,   100] loss: 0.015\n",
      "[10,   200] loss: 0.016\n",
      "[10,   300] loss: 0.021\n",
      "[10,   400] loss: 0.019\n",
      "[10,   500] loss: 0.021\n",
      "[10,   600] loss: 0.020\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "N = xtrain.size()[0] # Training set size\n",
    "B = 100 # Minibacth size\n",
    "NB = 600 # Number of minibatches\n",
    "T = 10 # Number of epochs\n",
    "gamma = .001 # learning rate\n",
    "rho = .9 # momentum\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr = gamma,\n",
    "                            momentum = rho)\n",
    "\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB) # shuffling\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k] # index of minibatch\n",
    "        # Extract i-th minibatch from xtrain and ltrain\n",
    "        idxsmp = range(i*B,i*B+B) # indices of samples for i-th minibatch\n",
    "        inputs = xtrain[idxsmp]\n",
    "        labels = ltrain[idxsmp]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # Initialize the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = net(inputs)\n",
    "        # Error evaluation\n",
    "        loss = criterion(outputs,labels.long())\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "        # Print averaged loss per minibatch every 100 mini - batches\n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f'%\n",
    "                    (epoch + 1,k + 1,running_loss*1.0/100))\n",
    "            running_loss = 0.0\n",
    "print ('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.86\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    xtest = xtest.cuda()\n",
    "\n",
    "yinit = net(xtest)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(100*np.mean(ltest == yinit.cpu().data.numpy().T.argmax(axis=0)))\n",
    "else:\n",
    "    print(100*np.mean(ltest == yinit.data.numpy().T.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved from 10.32% accuracy to 98.86% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
