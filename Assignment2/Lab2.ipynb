{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The shape of xtrain is 784 * 60000\n",
    "\n",
    "The shape of ltrain is 60000 * 1\n",
    "\n",
    "The size of training dataset is 60000\n",
    "\n",
    "The feature dimension is 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADLRJREFUeJzt3W+oXPWdx/HPx258YBJjbK6XYLV3V/KkFJosg6xWF6W0uIL/nvgPSwLS+KDCigX/PmgeiMhSLT5YhNiE3hR1W1AxoGTrJgXpk9BJiEk0trblynpzvZmgcA2EtNHvPpiTcjfeOTPOnJkz6ff9gmHOnO85OV+Ofu6Zc34zZxwRApDPeXU3AKAehB9IivADSRF+ICnCDyRF+IGkagm/7Rts/972H20/UkcPndiesX3I9gHbzZp72W77mO3Di+ZdbPtN2+8Xz6vHqLcttmeLfXfA9o019XaZ7d/Yftf2O7b/vZhf674r6auW/eZRj/Pb/oqkP0j6rqQPJf1O0l0R8e5IG+nA9oykRkQcH4Ne/lXSCUk7IuKbxbz/kPRxRDxV/OFcHREPj0lvWySdiIifjLqfs3pbK2ltROy3vVLSPkm3StqkGvddSV+3q4b9VseR/0pJf4yIP0fEXyT9l6Rbauhj7EXEW5I+Pmv2LZKmi+lptf/nGbkOvY2FiJiLiP3F9KeSjki6VDXvu5K+alFH+C+V9L+LXn+oGnfAEkLSr23vs7257maWMBkRc8X0R5Im62xmCffbPlicFtRySrKY7SlJGyTt1Rjtu7P6kmrYb1zw+6JrIuKfJf2bpB8Wb2/HUrTP2cbp89nPSbpC0npJc5KerrMZ2yskvSzpgYhYWFyrc98t0Vct+62O8M9KumzR668V88ZCRMwWz8ckvar2aco4mS/OHc+cQx6ruZ+/iYj5iPgsIj6X9Lxq3He2l6kdsBci4pVidu37bqm+6tpvdYT/d5LW2f5H2+dLulPSzhr6+ALby4sLMbK9XNL3JB0uX2vkdkraWExvlPRajb38P2eCVbhNNe0725a0TdKRiHhmUanWfdepr9r2W0SM/CHpRrWv+P9J0uN19NChr3+S9HbxeKfu3iS9pPbbwL+qfW3kXklflbRb0vuS/kfSxWPU2y8kHZJ0UO2gra2pt2vUfkt/UNKB4nFj3fuupK9a9tvIh/oAjAcu+AFJEX4gKcIPJEX4gaQIP5BUreEf04/PShrf3sa1L4ne+lVXb3Uf+cf2P4jGt7dx7Uuit36lDD+Amgz0IR/bN0h6VtJXJP0sIp4qW37NmjUxNTX1t9etVksTExN9b3+YxrW3ce1Lord+VdnbzMyMjh8/7l6W/Yd+N1LclOM/teimHLZ3RslNOaamptRs1npzHODvWqPR6HnZQd72c1MO4Bw2SPjH/aYcAEoM/YKf7c22m7abrVZr2JsD0KNBwt/TTTkiYmtENCKiMa4XXICMBgn/2N6UA0B3fV/tj4jTtu+X9N9qD/Vtj4h3KusMwFD1HX5Jiog3JL1RUS8ARohP+AFJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSGugnum3PSPpU0meSTkdEo4qmAAzfQOEvXB8Rxyv4dwCMEG/7gaQGDX9I+rXtfbY3V9EQgNEY9G3/NRExa/sSSW/afi8i3lq8QPFHYbMkXX755QNuDkBVBjryR8Rs8XxM0quSrlxima0R0YiIxsTExCCbA1ChvsNve7ntlWemJX1P0uGqGgMwXIO87Z+U9KrtM//OixGxq5KuAAxd3+GPiD9L+laFvQAYIYb6gKQIP5AU4QeSIvxAUoQfSKqKL/bgHBYRpfUTJ06U1nftKh/d3bFjR8fa22+/XbruoUOHSuurVq0qraMcR34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIpx/r8DCwsLHWt79uwpXXfbtm2l9ddff72vnnqxfPny0vqyZcuGtm1w5AfSIvxAUoQfSIrwA0kRfiApwg8kRfiBpBjnHwNHjx4trT/55JOl9bKx+lOnTpWuu27dutL6li1bSuunT58urT/xxBMda3fccUfpuhdccEFpHYPhyA8kRfiBpAg/kBThB5Ii/EBShB9IivADSTHOX4H33nuvtH7zzTeX1mdnZ0vrJ0+eLK0/+uijHWubNm0qXXdqaqq03u079d16Lxvn37BhQ+m6GK6uR37b220fs3140byLbb9p+/3iefVw2wRQtV7e9v9c0g1nzXtE0u6IWCdpd/EawDmka/gj4i1JH581+xZJ08X0tKRbK+4LwJD1e8FvMiLmiumPJE12WtD2ZttN281Wq9Xn5gBUbeCr/dH+pceOv/YYEVsjohERjYmJiUE3B6Ai/YZ/3vZaSSqej1XXEoBR6Df8OyVtLKY3SnqtmnYAjErXcX7bL0m6TtIa2x9K+rGkpyT9yva9kj6QdPswmxx3n3zySWn92muvLa2vWLGitH7PPfeU1huNRsea7dJ169Ttvv0Yrq7hj4i7OpS+U3EvAEaIj/cCSRF+ICnCDyRF+IGkCD+QFF/prcBVV101UP1c9vDDD/e97p133llhJ/iyOPIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKM82MgMzMzdbeAPnHkB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOfHUF1//fUda+eff/4IO8HZOPIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKM86PUwsJCaX3fvn2l9U2bNnWsnXcex546dd37trfbPmb78KJ5W2zP2j5QPG4cbpsAqtbLn96fS7phifk/jYj1xeONatsCMGxdwx8Rb0n6eAS9ABihQU667rd9sDgtWN1pIdubbTdtN1ut1gCbA1ClfsP/nKQrJK2XNCfp6U4LRsTWiGhERGNiYqLPzQGoWl/hj4j5iPgsIj6X9LykK6ttC8Cw9RV+22sXvbxN0uFOywIYT13H+W2/JOk6SWtsfyjpx5Kus71eUkiakXTfEHtEjfbs2VNaP3XqVGn9wQcfrLIdVKhr+CPiriVmbxtCLwBGiI9YAUkRfiApwg8kRfiBpAg/kBRf6UWp3bt3l9a7fS33kksuqbIdVIgjP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kxTg/Sh09erS0fvXVV5fWV61aVWU7qBBHfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iql5/ovkzSDkmTav8k99aIeNb2xZJ+KWlK7Z/pvj0iPhleqxiGbj+xvWvXrtL6TTfdVGU7GKFejvynJf0oIr4h6V8k/dD2NyQ9Iml3RKyTtLt4DeAc0TX8ETEXEfuL6U8lHZF0qaRbJE0Xi01LunVYTQKo3pc657c9JWmDpL2SJiNirih9pPZpAYBzRM/ht71C0suSHoiIhcW1iAi1rwcstd5m203bzVarNVCzAKrTU/htL1M7+C9ExCvF7Hnba4v6WknHllo3IrZGRCMiGhMTE1X0DKACXcNv25K2SToSEc8sKu2UtLGY3ijpterbAzAsvdy6+9uSvi/pkO0DxbzHJD0l6Ve275X0gaTbh9Mihmnv3r2l9ZMnT5bWH3rooSrbwQh1DX9E/FaSO5S/U207AEaFT/gBSRF+ICnCDyRF+IGkCD+QFOEHkuInupObnp7uvlCJyUm+0nGu4sgPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kxzo9SF110UWn9wgsvHFEnqBpHfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IinH+5Pbv319a7/YrSytXrqyyHYwQR34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKrrOL/tyyTtkDQpKSRtjYhnbW+R9ANJrWLRxyLijWE1iv68+OKLpfUDBw6U1h9//PEq28EY6eVDPqcl/Sgi9tteKWmf7TeL2k8j4ifDaw/AsHQNf0TMSZorpj+1fUTSpcNuDMBwfalzfttTkjZI2lvMut/2Qdvbba+uuDcAQ9Rz+G2vkPSypAciYkHSc5KukLRe7XcGT3dYb7Ptpu1mq9VaahEANegp/LaXqR38FyLiFUmKiPmI+CwiPpf0vKQrl1o3IrZGRCMiGt2+JAJgdLqG37YlbZN0JCKeWTR/7aLFbpN0uPr2AAxLL1f7vy3p+5IO2T4zLvSYpLtsr1d7+G9G0n1D6RADmZ+fH2j9u+++u6JOMG56udr/W0leosSYPnAO4xN+QFKEH0iK8ANJEX4gKcIPJEX4gaQcESPbWKPRiGazObLtAdk0Gg01m82lhua/gCM/kBThB5Ii/EBShB9IivADSRF+ICnCDyQ10nF+2y1JH4xsg0A+X4+Inm6ZNdLwAxgfvO0HkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS+j+uaNxGA6PvtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:, 42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The mage with index 42 corresponded to its label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(np.min(xtrain))\n",
    "print(np.max(xtrain))\n",
    "print(type(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    return ((x-127.5)/127.5).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(0, lbl.size)] = 1\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all(ltrain==onehot2label(dtrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    div = np.exp(a - a.max(axis=0))\n",
    "    return div / div.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Left Side\n",
    "\n",
    "= $ \\frac{exp(a_{i}) * \\sum_{j=1}^{10} exp(a_{j})-(exp(a_{i})^2}{(\\sum_{j=1}^{10} exp(a_{j}))^2} $\n",
    "\n",
    "= $ g(a)_{i} - (g(a)_{i})^2 $\n",
    "\n",
    "= $ g(a)_{i}(1-g(a)_{i}) $\n",
    "\n",
    "= Right Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Left Side\n",
    "\n",
    "= $ \\frac{-exp(a_{i}) * exp(a_{j})}{(\\sum_{j=1}^{10} exp(a_{j}))^2} $\n",
    "\n",
    "= $ -g(a)_{i} * g(a)_{j} (i \\neq j) $\n",
    "\n",
    "= Right Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    g = softmax(a)\n",
    "    return g*e-(g*e).sum(axis=0)*g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.924953639781273e-07, 'should be smaller than 1e-6')\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a+eps*e)-softmax(a))/eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return (a>0)*a\n",
    "def relup(a, e):\n",
    "    return (a>0)*e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "saved_init_net = copy.deepcopy(netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27118522872851974, 'should be around .26')\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    return -np.sum(d*np.log(y))/d.size\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9243833333333333\n"
     ]
    }
   ],
   "source": [
    "# This is the function for calculating prediction error rate\n",
    "# Lower error rate indicates better model performance\n",
    "def eval_perfs(y, lbl):\n",
    "    return np.sum(np.not_equal(np.argmax(y,axis=0),lbl))*1.0/lbl.size\n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # Forward Process\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    # Calculate delta\n",
    "    d2 = softmaxp(a2, -d/y)\n",
    "    d1 = relup(a1, W2.T.dot(d2))\n",
    "    # Gradient Descent\n",
    "    W2 -= gamma*d2.dot(h1.T)\n",
    "    W1 -= gamma*d1.dot(x.T)\n",
    "    b2 -= gamma*d2.sum(axis=1).reshape(No,1)\n",
    "    b1 -= gamma*d1.sum(axis=1).reshape(Nh,1)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        net = update_shallow(x,d,net,gamma)\n",
    "        if t % 9 == 0:\n",
    "            ypred = forwardprop_shallow(x,net)\n",
    "            loss = eval_loss(ypred,d)\n",
    "            acc = eval_perfs(ypred,lbl)\n",
    "            print(\"step= \"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 0.2430193361126099 error_rate= 0.8878666666666667\n",
      "step= 9 loss= 0.16869965299928635 error_rate= 0.5073333333333333\n",
      "step= 18 loss= 0.1279906442613118 error_rate= 0.3253666666666667\n",
      "step= 27 loss= 0.09777314424609186 error_rate= 0.26998333333333335\n",
      "step= 36 loss= 0.07894282861651937 error_rate= 0.19198333333333334\n",
      "step= 45 loss= 0.0785060681980576 error_rate= 0.22806666666666667\n",
      "step= 54 loss= 0.06528554301666793 error_rate= 0.18001666666666666\n",
      "step= 63 loss= 0.05793436021863722 error_rate= 0.1541\n",
      "step= 72 loss= 0.05416872516005859 error_rate= 0.14478333333333335\n",
      "step= 81 loss= 0.05144506038285361 error_rate= 0.13893333333333333\n",
      "step= 90 loss= 0.04927710173328055 error_rate= 0.13403333333333334\n",
      "step= 99 loss= 0.04748423392766081 error_rate= 0.12971666666666667\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, T=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.04489039280379487 error_rate= 0.1209\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,nettrain)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.03489616399533046 error_rate= 0.1109\n",
      "epoch= 1 loss= 0.026066686567975405 error_rate= 0.08073333333333334\n",
      "epoch= 2 loss= 0.021810754739571583 error_rate= 0.06761666666666667\n",
      "epoch= 3 loss= 0.019421813204439177 error_rate= 0.0605\n",
      "epoch= 4 loss= 0.017647700578601194 error_rate= 0.054933333333333334\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, (N+B-1)/B):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx],d[:,idx],net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        acc = eval_perfs(y,lbl)\n",
    "        print(\"epoch= \"+str(t)+\" loss= \"+str(loss)+\" error_rate= \"+str(acc))\n",
    "    return net\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_minibatch = init_shallow(Ni, Nh, No)\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit_minibatch, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.0176970714010138 error_rate= 0.0552\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,netminibatch)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Compared to the results of non-minibatch training, the minibatch training process is more effective and efficient for this task.\n",
    "\n",
    "We can even achieve lower error rate with less training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with network topology and learning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer --- 16 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 0.24175236568455205 error_rate= 0.8944166666666666\n",
      "step= 9 loss= 0.20831347299780903 error_rate= 0.7370333333333333\n",
      "step= 18 loss= 0.1751443833862672 error_rate= 0.5445666666666666\n",
      "step= 27 loss= 0.1747082363623221 error_rate= 0.5669\n",
      "step= 36 loss= 0.13796551781921465 error_rate= 0.45866666666666667\n",
      "step= 45 loss= 0.11618259002010299 error_rate= 0.35683333333333334\n",
      "step= 54 loss= 0.1092621518476823 error_rate= 0.3569833333333333\n",
      "step= 63 loss= 0.09203620442943036 error_rate= 0.2847\n",
      "step= 72 loss= 0.08407570109825377 error_rate= 0.2435\n",
      "step= 81 loss= 0.0769016282224372 error_rate= 0.2442\n",
      "step= 90 loss= 0.06924124716284287 error_rate= 0.18746666666666667\n",
      "step= 99 loss= 0.06375287815778367 error_rate= 0.18015\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 16\n",
    "No = dtrain.shape[0]\n",
    "netinit_16 = init_shallow(Ni, Nh, No)\n",
    "nettrain_16 = backprop_shallow(xtrain, dtrain, netinit_16, T=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.061824542327798035 error_rate= 0.1744\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,nettrain_16)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer --- 256 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 0.2574284472677842 error_rate= 0.8536666666666667\n",
      "step= 9 loss= 0.13875183397055119 error_rate= 0.3358833333333333\n",
      "step= 18 loss= 0.10322581516892267 error_rate= 0.29505\n",
      "step= 27 loss= 0.08380603212149584 error_rate= 0.2381\n",
      "step= 36 loss= 0.07142643231639065 error_rate= 0.2047\n",
      "step= 45 loss= 0.06272336499522241 error_rate= 0.17456666666666668\n",
      "step= 54 loss= 0.05772220449036079 error_rate= 0.1579\n",
      "step= 63 loss= 0.05331030798594166 error_rate= 0.14643333333333333\n",
      "step= 72 loss= 0.05031857174278993 error_rate= 0.13521666666666668\n",
      "step= 81 loss= 0.0477361181356213 error_rate= 0.13061666666666666\n",
      "step= 90 loss= 0.04578811171659701 error_rate= 0.12305\n",
      "step= 99 loss= 0.04415729540817198 error_rate= 0.12135\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 256\n",
    "No = dtrain.shape[0]\n",
    "netinit_256 = init_shallow(Ni, Nh, No)\n",
    "nettrain_256 = backprop_shallow(xtrain, dtrain, netinit_256, T=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.04182320798556537 error_rate= 0.1141\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,nettrain_256)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "As number of hidden neurons increases from 16, 64 to 256, the model is getting better. \n",
    "\n",
    "Because the loss is going down and the error rate is going down after same number of training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate --- 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 0.22939790575002703 error_rate= 0.8465833333333334\n",
      "step= 9 loss= 0.19524832126227606 error_rate= 0.6226166666666667\n",
      "step= 18 loss= 0.17022049864994773 error_rate= 0.47763333333333335\n",
      "step= 27 loss= 0.14933730564077774 error_rate= 0.38326666666666664\n",
      "step= 36 loss= 0.13198289926954745 error_rate= 0.31756666666666666\n",
      "step= 45 loss= 0.11778348077432663 error_rate= 0.2749666666666667\n",
      "step= 54 loss= 0.10630154344266261 error_rate= 0.24683333333333332\n",
      "step= 63 loss= 0.09701361255187078 error_rate= 0.22625\n",
      "step= 72 loss= 0.08952575633558278 error_rate= 0.2109\n",
      "step= 81 loss= 0.08343345759444362 error_rate= 0.19883333333333333\n",
      "step= 90 loss= 0.07841096551047062 error_rate= 0.18803333333333333\n",
      "step= 99 loss= 0.07421686268256289 error_rate= 0.18073333333333333\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_rate_2 = init_shallow(Ni, Nh, No)\n",
    "net_rate_2 = backprop_shallow(xtrain, dtrain, netinit_rate_2, T=100, gamma=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.07171615919625568 error_rate= 0.1691\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,net_rate_2)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate --- 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 0.2596018604796745 error_rate= 0.9005333333333333\n",
      "step= 9 loss= 0.17398080108040934 error_rate= 0.62585\n",
      "step= 18 loss= 0.12968476965788703 error_rate= 0.37776666666666664\n",
      "step= 27 loss= 0.09220647576151586 error_rate= 0.23083333333333333\n",
      "step= 36 loss= 0.07496457109781733 error_rate= 0.22413333333333332\n",
      "step= 45 loss= 0.06600055210112958 error_rate= 0.18598333333333333\n",
      "step= 54 loss= 0.06728782166276785 error_rate= 0.21426666666666666\n",
      "step= 63 loss= 0.055450686745516856 error_rate= 0.16928333333333334\n",
      "step= 72 loss= 0.050364439015061685 error_rate= 0.14673333333333333\n",
      "step= 81 loss= 0.04816757891444816 error_rate= 0.14211666666666667\n",
      "step= 90 loss= 0.04572295799456231 error_rate= 0.13073333333333334\n",
      "step= 99 loss= 0.04363880286458485 error_rate= 0.12321666666666667\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_rate_8= init_shallow(Ni, Nh, No)\n",
    "net_rate_8 = backprop_shallow(xtrain, dtrain, netinit_rate_8, T=100, gamma=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.0416235876500553 error_rate= 0.1155\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,net_rate_8)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "As the learning rate increases from 0.02, 0.05 to 0.08, the model performance becomes better.\n",
    "\n",
    "Because in the results obtained after same steps of training, the loss and error rate are all going down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch --- 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.026885692268959416 error_rate= 0.08336666666666667\n",
      "epoch= 1 loss= 0.021642568990659146 error_rate= 0.06736666666666667\n",
      "epoch= 2 loss= 0.01910729397863608 error_rate= 0.0605\n",
      "epoch= 3 loss= 0.015705528835534048 error_rate= 0.049466666666666666\n",
      "epoch= 4 loss= 0.013521860648351557 error_rate= 0.04248333333333333\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_batch_50= init_shallow(Ni, Nh, No)\n",
    "netminibatch_50 = backprop_minibatch_shallow(xtrain, dtrain, netinit_batch_50, 5, B=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.01429047319706763 error_rate= 0.0426\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,netminibatch_50)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch --- 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.0382946689204892 error_rate= 0.11913333333333333\n",
      "epoch= 1 loss= 0.030711723758992913 error_rate= 0.09325\n",
      "epoch= 2 loss= 0.026506088427784718 error_rate= 0.08078333333333333\n",
      "epoch= 3 loss= 0.023644822998864324 error_rate= 0.07151666666666667\n",
      "epoch= 4 loss= 0.021352521388454025 error_rate= 0.0642\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_batch_200= init_shallow(Ni, Nh, No)\n",
    "netminibatch_200 = backprop_minibatch_shallow(xtrain, dtrain, netinit_batch_200, 5, B=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.02165998055815807 error_rate= 0.064\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,netminibatch_200)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "If we compare the result for the situation of minibatch = 50, 100 and 200 for trainging 5 epochs, the lower minibatch will bring better model performance after 5 epochs since it will bring lower loss value and lower error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.034438980714173006 error_rate= 0.10761666666666667\n",
      "epoch= 1 loss= 0.026749341134644694 error_rate= 0.08323333333333334\n",
      "epoch= 2 loss= 0.02191498933698583 error_rate= 0.06771666666666666\n",
      "epoch= 3 loss= 0.018869483358886083 error_rate= 0.05918333333333333\n",
      "epoch= 4 loss= 0.01700183106195469 error_rate= 0.053233333333333334\n",
      "epoch= 5 loss= 0.015189994678442894 error_rate= 0.047066666666666666\n",
      "epoch= 6 loss= 0.0138212475038886 error_rate= 0.041933333333333336\n",
      "epoch= 7 loss= 0.012773047033702554 error_rate= 0.03871666666666667\n",
      "epoch= 8 loss= 0.011728144344917974 error_rate= 0.035333333333333335\n",
      "epoch= 9 loss= 0.010939820244228172 error_rate= 0.03306666666666667\n",
      "epoch= 10 loss= 0.010253791025739317 error_rate= 0.031166666666666665\n",
      "epoch= 11 loss= 0.009620906466983447 error_rate= 0.02925\n",
      "epoch= 12 loss= 0.009121200154046149 error_rate= 0.028033333333333334\n",
      "epoch= 13 loss= 0.008645798449270327 error_rate= 0.0268\n",
      "epoch= 14 loss= 0.008161415212400304 error_rate= 0.0252\n",
      "epoch= 15 loss= 0.007821830633512834 error_rate= 0.024366666666666665\n",
      "epoch= 16 loss= 0.007461314640463504 error_rate= 0.023433333333333334\n",
      "epoch= 17 loss= 0.007107969983488426 error_rate= 0.022283333333333332\n",
      "epoch= 18 loss= 0.006756382177245348 error_rate= 0.021083333333333332\n",
      "epoch= 19 loss= 0.006564913454763549 error_rate= 0.020483333333333333\n",
      "epoch= 20 loss= 0.006289779860514755 error_rate= 0.019616666666666668\n",
      "epoch= 21 loss= 0.006104425765549807 error_rate= 0.0193\n",
      "epoch= 22 loss= 0.005959423292071391 error_rate= 0.018733333333333334\n",
      "epoch= 23 loss= 0.0058240916013900515 error_rate= 0.01865\n",
      "epoch= 24 loss= 0.005576358556488718 error_rate= 0.01785\n",
      "epoch= 25 loss= 0.005446369303787781 error_rate= 0.0173\n",
      "epoch= 26 loss= 0.005200332171899159 error_rate= 0.016566666666666667\n",
      "epoch= 27 loss= 0.0050774195936551065 error_rate= 0.016183333333333334\n",
      "epoch= 28 loss= 0.0049269072458840765 error_rate= 0.015966666666666667\n",
      "epoch= 29 loss= 0.004860664514420966 error_rate= 0.015733333333333332\n"
     ]
    }
   ],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit_batch_trial= init_shallow(Ni, Nh, No)\n",
    "netminibatch_trial = backprop_minibatch_shallow(xtrain, dtrain, netinit_batch_trial, 30, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      "loss= 0.009831804855568893 error_rate= 0.0294\n"
     ]
    }
   ],
   "source": [
    "ypred = forwardprop_shallow(xtest,netminibatch_trial)\n",
    "loss = eval_loss(ypred,dtest)\n",
    "acc = eval_perfs(ypred,ltest)\n",
    "print(\"Test Set:\")\n",
    "print(\"loss= \"+str(loss)+\" error_rate= \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "I tried some parameters and here I got the error rate of 2.9%. However, there are definitely many advacned techniques to optimize the neural network to get better result and faster training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have written another python notebook, in which I implemented a python class that support building any number of layers and any number of hidden neurons neural network. In addition, some optimization techniques are also included such as regularization, options for different optimizers (including momentum option) and options for different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code for this part is in another notebook named BonusPart.ipynb . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install python-mnist module and then you could try to use my code of this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
